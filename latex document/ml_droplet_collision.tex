\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
 \PassOptionsToPackage{numbers, compress}{natbib}
% before loading nips_2018

% ready for submission
% \usepackage{nips_2018}

% to compile a preprint version, e.g., for submission to arXiv, add
% add the [preprint] option:
 \usepackage[preprint]{nips_2018}

% to compile a camera-ready version, add the [final] option, e.g.:
% \usepackage[final]{nips_2018}

% to avoid loading the natbib package, add option nonatbib:
% \usepackage[nonatbib]{nips_2018}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{graphicx}
\usepackage{verbatim}
\usepackage{subcaption}
\bibliographystyle{unsrtnat}
\usepackage{multirow}
\usepackage{enumitem}
\setlist[enumerate]{itemsep=0mm}
\setlist[itemize]{itemsep=0mm}

\title{Data driven prediction of droplet collision outcomes}

% The \author macro works with any number of authors. There are two
% commands used to separate the names and addresses of multiple
% authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to
% break the lines. Using \AND forces a line break at that point. So,
% if LaTeX puts 3 of 4 authors names on the first line, and the last
% on the second line, try using \AND instead of \And before the third
% author name.

\author{
  Arpit Agarwal\thanks{I worked alone towards this project. I explained my reasons in the project proposal.} \\
  %Department of Mechanical Engineering\\
  %University of Wisconsin -- Madison\\
  \texttt{agarwal32@wisc.edu} \\
  %% examples of more authors
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \AND
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
}

\begin{document}
% \nipsfinalcopy is no longer used

\maketitle

\begin{abstract}
Predicting the outcome of liquid droplet collisions is an extensively studied phenomenon but the current physics based models for predicting the outcomes are poor (accuracy $\approx 43\%$). The key weakness of these models is their limited complexity. They only account for 3 features while there are many more relevant features that go unaccounted for. This limitation of traditional models can be easily overcome through machine learning modeling of the problem. In an ML setting this problem directly translates to a classification problem with 4 classes. Here we compile a large labelled dataset and tune different ML classifiers over this dataset. We evaluate the accuracy and robustness of the classifiers. ML classifiers, with accuracies over 90\%, significantly outperform the physics based models. Another key question we try to answer in this paper is whether existing knowledge of the physics based models can be exploited to boost the accuracy of the ML classifiers. We find that while this knowledge improves the accuracy slightly for small datasets, it is useless with larger datasets being available.
\end{abstract}

\section{Introduction}
\subsection{Motivation}
Droplet collisions are important to the dynamics of a liquid spray. Sprays of engineering relevance have a very high droplet density and therefore a lot of droplet collisions. For example, diesel sprays have been reported to have 10$^8$  collisions per cm$^3$ per microsecond. Collisions have a direct effect on the liquid drops as they change the droplet sizes and velocities. Due to the high frequency of droplet collisions, and their cascading effect on the system dynamics, it is important to accurately model the droplet collision phenomena.

The problem of modeling the outcome of droplet collisions has been worked on extensively by fluid dynamicists. The models proposed in literature for predicting the outcomes of droplet collisions are phenomenological. They are based simplified analyses of the governing fluid dynamics equations. \citet{evaluating_collision} evaluate the current state of the art in physics based models and report an accuracy of around 64\% on the data they considered. On considering additional data reported by \citet{sommerfeld2016modelling} this accuracy drops to around 43\%. The complexity of these models is limited by our understanding of fluid mechanics, and our cognitive limitations. Models of higher complexity are needed to improve the accuracy.

The problem of droplet collision prediction can be directly framed as a 4-class classification problem. This problem is ideal for a machine learning solution because of two reasons:
\begin{enumerate}
\item Ground truth complexity is high - beyond physical modeling
\item The problem is well studied, hence a lot of experimental data has been published over the years
\end{enumerate}
Here I compile labelled datasets from literature (7898 points in all, see table~\ref{tab:data}) and train machine learning classification models over this data. 



\subsection{Fluid Mechanics Background}
\label{sec:background}
Four main types of outcomes are expected from collisions of two droplets, these are illustrated in figure~\ref{fig:outcomes}. As this is a deterministic phenomenon, a priori prediction of collision outcomes is possible with enough information about the droplet and flow characteristics. Due to limitations of the techniques used, in current fluid mechanics literature, only three main non-dimensional quantities, $\Delta, We$ and $B$ are considered in the outcome of a binary collision. It is well known that more features are relevant, but no models exist that account for all relevant features.

\begin{figure}[h]
	\centering
	\includegraphics[width=0.8\textwidth]{../figures/outcome-illustration.png}
	\caption{An illustration of the 4 types of outcomes expected. These are the classification labels for the present study.}
	\label{fig:outcomes}
\end{figure}
As highlighed in fig.~\ref{fig:outcomes} the four class labels (collision outcomes) are:
\begin{enumerate}
\item Coalescence
\item Bouncing
\item Stretching separation (or off-center separation)
\item Reflexive separation (or head-on separation)
\end{enumerate}

As per the traditional modeling approach, the 3-dimensional feature space is divided into 4 regions corresponding to the respective outcomes. Figure~\ref{fig:munnannur} shows a slice of this 3-dimensional space ($\Delta$ is set to 1), showing the four different outcomes. This figure also gives an example of a typical labelled experimental dataset. 


% A sample dataset with the 4 labels and the traditional (fluid-mechanistic) classification curves. These simplified curves fail to capture all of the physics (they only consider 3 features) and hence perform poorly.

\begin{figure}[h!]
    \centering
    \begin{subfigure}[b]{0.45\textwidth}
        \includegraphics[width=\textwidth]{../figures/munnannur.png}
        \caption{A sample dataset with the 4 labels and the physics based classification curves}
        \label{fig:munnannur_a}
    \end{subfigure}
    ~ %add desired spacing between images, e. g. ~, \quad, \qquad, \hfill etc. 
      %(or a blank line to force the subfigure onto a new line)
    \begin{subfigure}[b]{0.45\textwidth}
        \includegraphics[width=\textwidth]{../figures/regimes.png}
        \caption{Classification curves \& map for the physics based models. The four different colors correspond to the four different labels.}
        \label{fig:regime}
    \end{subfigure}
    \caption{A sample dataset \cite{munnannur2007new} and the physics based classification map \cite{evaluating_collision}.}
    \label{fig:munnannur}
\end{figure}





\begin{comment}
\begin{enumerate}
\item Drop size ratio, $\Delta = d_s/d_l$,  where $d_s$ and $d_l$ are the diameters of the smaller and larger participating drops, respectively
\item	Drop Weber number, $We_D = \rho U^2 d_s/\sigma$, where $\rho, U$ and $\sigma$ are the liquid density, relative velocity and surface tension constant respectively
\item Impact parameter, $B$, which is the non-dimensionalized orthogonal distance between the interacting drops. It has been described in further detail in [8].
\end{enumerate}
\end{comment}

\clearpage
\section{Dataset}


Data has been collected from four published articles. Details of the references are provided in Table~\ref{tab:data}. I manually extracted 7898 data points from the sources listed in Table~\ref{tab:data} using a web-based tool~\cite{rohatgi2011webplotdigitizer}. Each data point corresponds to one collision experiment.

\begin{table}[h]
  \caption{Source datasets}
  \label{tab:data}
  \centering
  \begin{tabular}{c l c p{5cm}}
    \toprule
    Date  & Source reference      				& No. of data points  & Comments\\
    \midrule
    1990 & \citet{ashgriz1990coalescence}     		& 986   &  \multirow{3}{5cm}{Early work - only basic features considered} \\
    1999 & \citet{estrade1999experimental} 		& 905   & \\ 
    1997 & \citet{qian1997regimes}    			& 217   & \\
    2016 & \citet{sommerfeld2016modelling}  		& 5790 & Newer work - emphasis on features that are not represented in traditional models\\
    \cmidrule(r){3-3}
             & 									& Total: 7898 & \\
    \bottomrule
  \end{tabular}
\end{table}



All the input features are numerical. The 5 base features and 3 additional features are described in table~\ref{tab:features}. 

\begin{table}[h]
  \caption{Descriptions of the features}
  \label{tab:features}
  \centering
  \begin{tabular}{l l c p{5cm}}
    \toprule
    & 							Features		  			& Range & Comments\\
    \midrule
   \multirow{ 5}{*}{Base Features} 	& $We$ (Weber number)      	& $\rm I\!R^+$ 	& See fig.~\ref{fig:data0}  \\
     							& $B$ (Impact param.)		& [0, 1]  			& See fig.~\ref{fig:data0} \\
    							& $\Delta$ (Drop size ratio)  	& [0, 1]  			& Most data has $\Delta = 1$; a few points with $\Delta = \{0.5, 0.75\}$ (see fig. \ref{fig:data1})   \\
    							& $P$ (Gas pressure) 		& $\rm I\!R^+$	&  Only discrete training data available (see fig.~\ref{fig:data2})\\
    							& $\mu$ (Liquid viscosity) 		& $\rm I\!R^+$ 	& Only discrete training data available (see fig.~\ref{fig:data3}) \\
   \midrule
   \multirow{ 3}{3cm}{Additional Features (combinations of base features)} 	& Curve $C_1$     			& $\rm I\!R$ &  see eq.~\ref{eq:c1}\\
     							& Curve $C_2$					& $\rm I\!R$  & see eq.~\ref{eq:c2} \\
    							& Curve $C_3$  					& $\rm I\!R$  & see eq.~\ref{eq:c3}\\
    \bottomrule
  \end{tabular}
\end{table}


\begin{figure}[h]
	\centering
	\includegraphics[width=0.7\textwidth]{../figures/data_scatterhist.png}
	\caption{Data}
	\label{fig:data0}
\end{figure}

\begin{figure}[h!]
    \centering
    \begin{subfigure}[b]{0.4\textwidth}
        \includegraphics[width=\textwidth]{../figures/data_scatterhist1.png}
        \caption{Map w.r.t. features 1 and 3 }
        \label{fig:data1}
    \end{subfigure}
    ~ %add desired spacing between images, e. g. ~, \quad, \qquad, \hfill etc. 
      %(or a blank line to force the subfigure onto a new line)
    \begin{subfigure}[b]{0.4\textwidth}
        \includegraphics[width=\textwidth]{../figures/data_scatterhist2.png}
        \caption{Map w.r.t. features 1 and 4 }
        \label{fig:data2}
    \end{subfigure}
    ~ %add desired spacing between images, e. g. ~, \quad, \qquad, \hfill etc. 
    %(or a blank line to force the subfigure onto a new line)
    \begin{subfigure}[b]{0.4\textwidth}
        \includegraphics[width=\textwidth]{../figures/data_scatterhist3.png}
        \caption{Map w.r.t. features 1 and 5 }
        \label{fig:data3}
    \end{subfigure}
        ~ %add desired spacing between images, e. g. ~, \quad, \qquad, \hfill etc. 
    %(or a blank line to force the subfigure onto a new line)
    \begin{subfigure}[b]{0.4\textwidth}
        \includegraphics[width=\textwidth]{../figures/data_scatterhist4.png}
        \caption{Map w.r.t. features 5 and 2 }
        \label{fig:data4}
    \end{subfigure}
    \caption{Different maps of the dataset}\label{fig:animals}
\end{figure}

\subsection{Incorporating domain knowledge}
Additional features formed through combinations of base features.
Decision curve in physics based models \cite{munnannur2007new} (see figure \ref{fig:regime}) incorporated as features as shown here :
\begin{equation}
\label{eq:c1}
 C_1 = We - \frac{\Delta(1+\Delta^2)(4\Phi - 12)}{\chi_1\left[ \cos\left( \sin^{-1}B\right) \right]}
\end{equation}
\begin{equation}
\label{eq:c2}
 C_2 = We - \frac{3[7(1+\Delta^3)^{2/3} - 4(1+\Delta^2)]\Delta(1+\Delta^3)^2}{\Delta^6 \eta_1 + \eta_2}
\end{equation}
\begin{equation}
\label{eq:c3}
C_3: B - \sqrt{\frac{2.4(\Delta^3- 2.4 \Delta^2 + 2.7\Delta)}{We}}
\end{equation}

Three such decision curves form the physics based model.

\subsection{Gaps in the data set}
The traditional physics based models only consider 3 features ($\Delta, We$, $B$), therefore, in reporting the data some authors do not report values for the other two features ($P, \mu$) considered here. To fill the gaps I have made educated guesses of those features. Furthermore, \citet{qian1997regimes} do not distinguish between classes 3 and 4, and report them as the same. Here, again, I have made educated guesses for their labels.

\section{Machine Learning Modeling}
Scikit-learn \cite{scikit_learn} and numpy are the two main libraries used through this work. All the machine learning model implementations are used directly from scikit-learn. As part of preprocessing the dataset is shuffled randomly and the feature values are standardized (scaled  to a mean of 0 and standard deviation of 1). \textbf{Ten fold cross validation results have been reported throughout the paper}.

%Section~\ref{sec:tuning} describes the hyperparameter tuning.\\

\subsection{Hyperparameter tuning}
\label{sec:tuning}
In this section we tune hyperparameters for a few non-linear classifiers. For each of the classifiers, we consider two settings:
\begin{enumerate} 
\item Simply use the 5 base features, i.e. no domain knowledge used
\item Use the 3 extra features, i.e. leverage domain knowledge
\end{enumerate}

\subsubsection{Decision Trees}
Here we test two different splitting criterions over different values of maximum tree depth. A few observations can be made:
\begin{itemize}
\item The 3 additional features help significantly when the trees are very short. The extra features play a diminishing role in accuracy as the trees get deeper. The accuracy values for both the cases (fig. \ref{fig:dt_wo} and fig. \ref{fig:dt_w}) saturate around 0.93.
\item Choice of splitting criterion (gini impurity vs. entropy) has little effect on accuracy.
\item Accuracy saturates beyond max tree depth of around 20. This value remains similar with 5 input features and 8 input features. This is confirmed in experiments with random forests as well (see figure~\ref{fig:rf}).
\end{itemize}

\begin{figure}[h!]
    \centering
    \begin{subfigure}[b]{0.45\textwidth}
        \includegraphics[width=\textwidth]{../figures/wo_domain_knowledge/accuracy_decision_tree.png}
        \caption{Without domain knowledge (5  features)}
        \label{fig:dt_wo}
    \end{subfigure}
    ~ %add desired spacing between images, e. g. ~, \quad, \qquad, \hfill etc. 
      %(or a blank line to force the subfigure onto a new line)
    \begin{subfigure}[b]{0.45\textwidth}
        \includegraphics[width=\textwidth]{../figures/w_domain_knowledge/accuracy_decision_tree.png}
        \caption{With domain knowledge (8  features)}
        \label{fig:dt_w}
    \end{subfigure}
    \caption{Cross-validation accuracy with decision trees}
    \label{fig:dt}
\end{figure}



\subsubsection{Random Forests}

Here we test accuracy vs. the number of estimators used. We do this for two different values of max tree depth. This is repeated with both cases, one with 5 features and one with the extra features. A few observations can be made from these tests:
\begin{itemize}
\item The accuracy values are fairly high (close to 0.95 for high number of estimators).
\item The extra features have no impact on the accuracy. Even for a small number of estimators. This makes sense considering the results seen in figure~\ref{fig:dt}. The accuracy of decision trees was not affected by the 3 additional features beyond a tree depth of 16. Here we consider tree depths of 16 and 32, therefore, even with 1 estimator, the accuracy values are not affected by the additional features.
\item The maximum tree depth values (beyond 16) has no impact on accuracy of the random forests.
\item The accuracy value is practically saturated beyond 30 estimators for all cases.
\end{itemize}
\begin{figure}[h!]
    \centering
    \begin{subfigure}[b]{0.45\textwidth}
        \includegraphics[width=\textwidth]{../figures/wo_domain_knowledge/accuracy_rand_forests.png}
        \caption{Without domain knowledge (5  features)}
        \label{fig:rf_wo}
    \end{subfigure}
    ~ %add desired spacing between images, e. g. ~, \quad, \qquad, \hfill etc. 
      %(or a blank line to force the subfigure onto a new line)
    \begin{subfigure}[b]{0.45\textwidth}
        \includegraphics[width=\textwidth]{../figures/w_domain_knowledge/accuracy_rand_forests.png}
        \caption{With domain knowledge (8  features)}
        \label{fig:rf_w}
    \end{subfigure}
    \caption{Cross-validation accuracy with random forests}\label{fig:rf}
\end{figure}

\subsubsection{K-Nearest Neighbors}
Here we experiment with different distance metrics ($L_1$ and $L_2$) and different label computation options (uniform and distance weighted averaging). These experiments are repeated with the 5 base features and with all the 8 features. Here are a few observations based on results in fig.~\ref{fig:knn}:
\begin{itemize}
\item Insignificant difference in accuracy when the additional features are considered.
\item Label calculation method directly impacts accuracy. With $k=1$, we have a single neighbor, therefore the distance weighted label coincides with the direct computation. 
\item As the $k$ value increases the uniformly weighted label accuracy drops quickly. $k=1$ is the optimal value for uniform weighing. This makes sense when we consider the data in fig.~\ref{fig:data0}. There is no gap between the different classes, moreover, there is some mixing of the classes as well. Therefore higher $k$ values pollute the labels.
\item This issue is remedied in the distance weighted labels, where closer points are weighted higher. Here we observe a small increase in accuracy with increasing $k$ values up to $k=30$.
\item Finally, the the $L_1$ and $L_2$ distance metrics have little impact on the accuracy.
\end{itemize}

\begin{figure}[h!]
    \centering
    \begin{subfigure}[b]{0.45\textwidth}
        \includegraphics[width=\textwidth]{../figures/wo_domain_knowledge/accuracy_knn.png}
        \caption{Without domain knowledge (5  features)}
        \label{fig:gull}
    \end{subfigure}
    ~ %add desired spacing between images, e. g. ~, \quad, \qquad, \hfill etc. 
      %(or a blank line to force the subfigure onto a new line)
    \begin{subfigure}[b]{0.45\textwidth}
        \includegraphics[width=\textwidth]{../figures/w_domain_knowledge/accuracy_knn.png}
        \caption{With domain knowledge (8  features)}
        \label{fig:tiger}
    \end{subfigure}
    \caption{Cross-validation accuracy with k-nn}\label{fig:knn}
\end{figure}

\subsubsection{Neural Networks}
Here we consider the accuracy of neural network classifiers as a function of number of training epochs/iterations. We tested out different neural network architectures (different number of hidden layers and neurons per layer) but they performed similarly with 2 or more hidden layers with more than 10 neurons each. In figure \ref{fig:nn} we present results with a 5 hidden layers, first four have 40 neurons and the last one has 8 neurons. Once again we see that the additional features have little impact on the accuracy. Also, the accuracy saturates beyond 300 iterations.

\begin{figure}[h!]
    \centering
    \begin{subfigure}[b]{0.45\textwidth}
        \includegraphics[width=\textwidth]{../figures/wo_domain_knowledge/accuracy_neural_nets.png}
        \caption{Without domain knowledge (5  features)}
        \label{fig:gull}
    \end{subfigure}
    ~ %add desired spacing between images, e. g. ~, \quad, \qquad, \hfill etc. 
      %(or a blank line to force the subfigure onto a new line)
    \begin{subfigure}[b]{0.45\textwidth}
        \includegraphics[width=\textwidth]{../figures/w_domain_knowledge/accuracy_neural_nets.png}
        \caption{With domain knowledge (8  features)}
        \label{fig:tiger}
    \end{subfigure}
    \caption{Cross-validation accuracy with Neural Networks}\label{fig:nn}
\end{figure}

\subsection{Accuracy with tuned models}
In this section we compare the accuracy of different models (with tuned hyperparameters) for the two cases (one with the 5 base features and the other with the additional features). The results are presented in fig.~\ref{fig:tuned}. The accuracy of the state of the art physics based models is also presented (in red) as a baseline. A few comments and observations regarding the results:
\begin{itemize}
\item Even the simplest ML models provide a significant improvement over the current state of the art physics based models. This is related to the low model complexity in the physics based models.
\item Little difference in the accuracy in the two cases. This is consistent with our observations in figures~\ref{fig:dt}--\ref{fig:nn}. The naive bayes classifier actually suffers with the additional features.
\item The random forest classifier performs the best on this dataset. Although it is only marginally better than the much simpler decision tree and k-nn classifiers.
\item The `ensemble' here refers to a majority vote of the random forest, neural network, SVM and k-NN predictions. This ensemble classifier does not boost the accuracy. An ensemble is expected to improve accuracy when the different classifiers exploit different aspects of the dataset.
\end{itemize}

\begin{figure}[h!]
    \centering
    \begin{subfigure}[b]{0.45\textwidth}
        \includegraphics[width=\textwidth]{../figures/wo_domain_knowledge/accuracy_compilation.png}
        \caption{Without domain knowledge (5  features)}
        \label{fig:gull}
    \end{subfigure}
    ~ %add desired spacing between images, e. g. ~, \quad, \qquad, \hfill etc. 
      %(or a blank line to force the subfigure onto a new line)
    \begin{subfigure}[b]{0.45\textwidth}
        \includegraphics[width=\textwidth]{../figures/w_domain_knowledge/accuracy_compilation.png}
        \caption{With domain knowledge (8  features)}
        \label{fig:tiger}
    \end{subfigure}
    \caption{Cross-validation accuracy with different models}\label{fig:tuned}
\end{figure}

The confusion matrix for the random forest classifier is presented in figure~\ref{fig:conf} as a sample. The accuracy is fairly high for the first 3 classes. The highest error is when the true label is `reflexive' and it gets classified as `coalescence'. More training data near this boundary can help boost the model accuracy further. This can possibly guide further experiments.
\begin{figure}[h]
	\centering
	\includegraphics[width=0.4\textwidth]{../figures/conf_mat.png}
	\caption{Confusion matrix for the random forest classifier.}
	\label{fig:conf}
\end{figure}

\subsection{Why did domain knowledge not help?}
A key question that arises from the experiments presented so far is why the domain knowledge in the form of the additional features did not help the performance of any of the models. To answer this we consider an experiment with increasing datasize. The two simple classifiers that perform well (fig. \ref{fig:tuned}), decision tree and k-NN, are considered. For both the classifiers we consider training with 5 features (dotted lines) and 8 features (solid lines). A few observations can be made:
\begin{itemize}
\item The accuracy has not saturated with respect to increasing number of data points. More data is expected to boost accuracy of our models further.
\item For higher number of data points (beyond 2000) the domain knowledge does not help, but there is a small difference for small numbers of data points. This is highlighted in figure~\ref{fig:acc_vs_size_b}, where we zoom in on the relevant region.
\item The improvement (albeit small) in performance by addition of the extra features suggests that when there is a small amount of training data the models are unable to search the entire hypothesis space. The addition of the relevant features allows a better accuracy even when the hypothesis space is not completely spanned.
\item This improvement vanishes as the number of training data points increases because the models are able to better search the hypothesis space with additional data.
\end{itemize}

\begin{figure}[h!]
    \centering
    \begin{subfigure}[b]{0.475\textwidth}
        \includegraphics[width=\textwidth]{../figures/accuracy_vs_size.png}
        \caption{Over the whole range}
        \label{fig:acc_vs_size_a}
    \end{subfigure}
    ~ %add desired spacing between images, e. g. ~, \quad, \qquad, \hfill etc. 
      %(or a blank line to force the subfigure onto a new line)
    \begin{subfigure}[b]{0.475\textwidth}
        \includegraphics[width=\textwidth]{../figures/accuracy_vs_size2.png}
        \caption{For small dataset sizes (zoomed in fig. \ref{fig:acc_vs_size_a})}
        \label{fig:acc_vs_size_b}
    \end{subfigure}
    \caption{Accuracy with different models as a function of number of data points.}\label{fig:acc_vs_size}
\end{figure}

\subsection{Robustness to noise}
A key metric to consider for the current application is the robustness to noise. As described in table~\ref{tab:data}, the training data has been extracted from different sources going as far back as 1990. Certain experimental errors (up to 10\%) have been reported in literature and some variations can be expected across the different datasets. Therefore, we must account for noise in our input data. To do this we run experiments where we add a varying degree of noise to the data.

Gaussian noise is added to the scaled feature vectors. The noisy data is given as 
\begin{equation}
X(i, j) = X(i, j) + N(i, j),
\end{equation}
where $N \sim \mathcal{N}(0, n)$, $i$ is the instance index and $j$ is the feature number. The value of $n$ is represented on the x-axis in fig.~\ref{fig:noise}. Four different classifiers are compared here. Two key things emerge from this test:
\begin{itemize}
\item The decision tree classifier is the least robust to noise. There is a sharp drop in accuracy initially up to 5\% noise.
\item The neural network classifier, while starting from a lowest accuracy is the most robust to input noise. The neural network performs even better than the random forest for high levels of noise.
\end{itemize}

\begin{figure}[h]
	\centering
	\includegraphics[width=0.7\textwidth]{../figures/accuracy_with_noise.png}
	\caption{Accuracy of different classifiers as a function of noise in the data. The $n$ value denotes the standard deviation in the Gaussian noise. }
	\label{fig:noise}
\end{figure}

\section{Conclusions}

A major improvement is seen over the current state of the art physics based models ($\approx 43\%$) with all ML classifiers studied here ($> 90\%$). The accuracy is more than doubled. The best classifiers for the current dataset based on three key metrics are:
\begin{itemize}
\item Accuracy: Random forests
\item Interpretability: Decision tree
\item Robustness: Neural Networks
\end{itemize}
The k-NN classifier performs decently well on all three criteria.

In the confusion matrix (fig. \ref{fig:conf}) the highest error is seen when the true label is `reflexive' and it gets classified as `coalescence'. More training data near this boundary can help boost the model accuracy further.

This is a relevant problem for a lot of industries. This paper demonstrates that ML based classifiers can significantly improve predictions but apart from simply improving the model accuracy, this work can be leveraged further to provide insights into the fluid mechanics of the problem itself. The next step here would be to interpret these ML based models (mainly the decision tree) and try to gain insights about the dependence of the droplet collision phenomena on the different features.

%Shuffling of data before generating cross validation data sets is important.



\bibliography{ml_droplet_collision}
\end{document}
